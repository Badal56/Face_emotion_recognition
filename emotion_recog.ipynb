{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Face_emotion_recog.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1E54OZ81BPldkVYkfccNWwZa9KCUG9sxS","authorship_tag":"ABX9TyPPXcAJ5tQW0oT1usBqWrT4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xnk-kD9w-wyl","executionInfo":{"status":"ok","timestamp":1635174373090,"user_tz":-330,"elapsed":11740,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}},"outputId":"7f16f781-7eaa-4f8a-dbc8-d7288ff99194"},"source":["!git clone https://github.com/muxspace/facial_expressions.git"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'facial_expressions'...\n","remote: Enumerating objects: 14214, done.\u001b[K\n","remote: Total 14214 (delta 0), reused 0 (delta 0), pack-reused 14214\u001b[K\n","Receiving objects: 100% (14214/14214), 239.65 MiB | 35.48 MiB/s, done.\n","Resolving deltas: 100% (223/223), done.\n","Checking out files: 100% (13996/13996), done.\n"]}]},{"cell_type":"code","metadata":{"id":"nO5XlMwE-_YA"},"source":["import csv\n","data={}\n","with open('/content/facial_expressions/data/legend.csv') as f:\n","  reader=csv.reader(f)\n","  next(reader)\n","  for row in reader:\n","    key=row[2].lower()\n","    if key in data:\n","      data[key].append(row[1])\n","    else:\n","      data[key]=[row[1]]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdpiV141_5hk"},"source":["emotion_list=list(data.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veJ2mjC6AQ1g"},"source":["import os\n","os.mkdir('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data')\n","os.mkdir('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/training')\n","os.mkdir('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/testing')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSSeW8a1Al9m"},"source":["for emotions in emotion_list:\n","  os.mkdir(os.path.join('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/training/',emotions))\n","  os.mkdir(os.path.join('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/testing/',emotions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQ2AtFdUA5H_"},"source":["from shutil import copyfile\n","split_size=0.8\n","for emotion,images in data.items():\n","  train_size=int(split_size*len(images))\n","  train_images=images[:train_size]\n","  test_images=images[train_size:]\n","  for image in train_images:\n","    source=os.path.join('/content/facial_expressions/images',image)\n","    des=os.path.join('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/training',emotion,image)\n","    copyfile(source,des)\n","  for image in test_images:\n","    source=os.path.join('/content/facial_expressions/images',image)\n","    des=os.path.join('/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/testing',emotion,image)\n","    copyfile(source,des)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpWR-mXLCUkf","executionInfo":{"status":"ok","timestamp":1635324874889,"user_tz":-330,"elapsed":2364,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import MaxPooling2D,Dense,Flatten,Conv2D"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"-cJKxrIcpwO5","executionInfo":{"status":"ok","timestamp":1635324878689,"user_tz":-330,"elapsed":1380,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}}},"source":["model=tf.keras.models.Sequential([\n","                                 Conv2D(16,(3,3),activation='relu',input_shape=(100,100,3)),\n","                                 MaxPooling2D(2,2),\n","                                 Conv2D(32,(3,3),activation='relu'),\n","                                 MaxPooling2D(2,2),\n","                                 Conv2D(64,(3,3),activation='relu'),\n","                                 MaxPooling2D(2,2),\n","                                 Flatten(),\n","                                 Dense(512,activation='relu'),\n","                                 Dense(8,activation='softmax')\n","])"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCHaubCcq8TQ","executionInfo":{"status":"ok","timestamp":1635324886493,"user_tz":-330,"elapsed":986,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}},"outputId":"1e7803ae-fb7d-4000-c801-441aaab9831f"},"source":["model.compile(optimizer=Adam(lr=0.01),loss='categorical_crossentropy',metrics=['acc'])"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAHw4TvSrmpB","executionInfo":{"status":"ok","timestamp":1635324887326,"user_tz":-330,"elapsed":14,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}},"outputId":"741639cd-6298-41f0-833f-0e9de076c79f"},"source":["model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 98, 98, 16)        448       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 49, 49, 16)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 47, 47, 32)        4640      \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 23, 23, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 21, 21, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 10, 10, 64)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 6400)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               3277312   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 8)                 4104      \n","=================================================================\n","Total params: 3,305,000\n","Trainable params: 3,305,000\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"ALH2tagYrs9q","executionInfo":{"status":"ok","timestamp":1635324888695,"user_tz":-330,"elapsed":2,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}}},"source":["train_dir='/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/training'\n","test_dir='/content/drive/MyDrive/M.L projects/facial_recognisation/master_data/testing'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_WskeeC-wjl9","executionInfo":{"status":"ok","timestamp":1635324890199,"user_tz":-330,"elapsed":600,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}},"outputId":"8798e32d-b19b-456e-bd47-f3d96cd3021d"},"source":["train_datagen=ImageDataGenerator(rescale=1.0/255)\n","train_generator=train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(100,100),\n","    class_mode='categorical',\n","    batch_size=128\n",")\n","test_datagen=ImageDataGenerator(rescale=1.0/255)\n","test_generator=test_datagen.flow_from_directory(\n","    test_dir,\n","    target_size=(100,100),\n","    class_mode='categorical',\n","    batch_size=500\n",")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 10941 images belonging to 8 classes.\n","Found 2742 images belonging to 8 classes.\n"]}]},{"cell_type":"code","metadata":{"id":"FjcjgmGEw_mA","executionInfo":{"status":"ok","timestamp":1635324890205,"user_tz":-330,"elapsed":16,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}}},"source":["es=EarlyStopping(monitor='val_acc',patience=5,min_delta=0.01)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"id":"W21x6vFYxMq7","executionInfo":{"status":"error","timestamp":1635325751606,"user_tz":-330,"elapsed":859330,"user":{"displayName":"Badal Jain","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15868985051341794590"}},"outputId":"154844f4-0ec7-4f50-b270-cb06fe1dd210"},"source":["model.fit_generator(train_generator,\n","                    epochs=20,\n","                    verbose=1,\n","                    validation_data=test_generator,\n","                    callbacks=[es])"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","28/86 [========>.....................] - ETA: 27:16 - loss: 2.1407 - acc: 0.4796"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-05e47a449e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     callbacks=[es])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}